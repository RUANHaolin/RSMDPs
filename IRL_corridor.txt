173
prob_accept = min(1.0, np.exp(prop_ll - curr_ll))



171
prop_ll = self.log_likelihood(proposal_weights, proposal_q_values, demos_sa)



154
curr_ll =  self.log_likelihood(curr_weights, curr_q_values, demos_sa)



152
curr_occupancies, curr_q_values = self.solve_optimal_policy(curr_weights)



107
reward_sa = self.mdp_env.transform_to_R_sa(reward_weights)


``mdp_worlds.py'' 5-25
def lava_ambiguous_corridor():
    num_rows = 3
    num_cols = 5
    num_states = num_rows * num_cols
    white = (1,0)
    red = (0,1)
    state_features = np.array([white, white, white, white, white,
                               white, red, red, red, red,
                               white, white, white, white, white])
    weights = np.array([-0.1, -0.9])#np.array([-0.26750391, -0.96355677])#np.array([-.18, -.82])
    weights = weights / np.linalg.norm(weights)
    print(weights)
    gamma = 0.99
    init_dist = np.zeros(num_states)
    # init_states = [5,4]
    # for si in init_states:
    #     init_dist[si] = 1.0 / len(init_states)
    term_states = [14]
    init_dist = 1/(num_states) * np.ones(num_states)
    mdp_env = mdp.FeaturizedGridMDP(num_rows, num_cols, state_features, weights, gamma, init_dist, term_states)
    return mdp_env


``mdp.py'' 400-412
    def transform_to_R_sa(self, reward_weights):
        #assumes that inputs are the reward feature weights or state rewards
        #returns the vectorized R_sa 
        
        #first get R_s
        if len(reward_weights) == self.get_reward_dimensionality():
            R_s = np.dot(self.state_features, reward_weights)
        elif len(reward_weights) == self.num_states:
            R_s = reward_weights
        else:
            print("Error, reward weights should be features or state rewards")
            sys.exit()
        return np.tile(R_s, self.num_actions)

sample_posterior流程(improper prior)：

获取demo -> 抽初始weight -> 用curr weight解curr occupancies和curr q 
-> 用curr q和demo算curr loglikelihood 
（loop start）
-> 用curr_weights生成proposal weights -> 用proposal weight解proposal occupancies和proposal q
-> 用proposal q和demo算proposal loglikelihood -> 用 proposal ll和current ll算accept prob.
(If accept proposal)--> 将proposal weights add to chain -> 将proposal occupancies frequencies add to chain -> 
                     -> curr ll, curr weights, curr occupancies用proposal ll, proposal weights, proposal occupancies替
                    (If proposal ll > best ll) best_ll = prop_ll, map_weights = proposal_weights, map_occupancy = proposal_occupancies
(If reject proposal)--> curr weights add to chain -> curr occupancies add to chain

